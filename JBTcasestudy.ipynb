{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LATITUDE3500-K98D0QU:4043\n",
       "SparkContext available as 'sc' (version = 2.4.8, master = local[*], app id = local-1625568717312)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+----------+------------+--------------+------------+------------+--------+------+\n",
      "|         booking_id|traveller_id|company_id|booking_date|departure_date|      origin| destination|priceUSD|status|\n",
      "+-------------------+------------+----------+------------+--------------+------------+------------+--------+------+\n",
      "|2188183561171387127|         334|        10|  2019-01-02|    2019-03-20|       Sofia|    Budapest|     725|BOOKED|\n",
      "|4323535508030040434|           8|         9|  2019-01-02|    2019-04-09|   New Delhi|      Sydney|      87|BOOKED|\n",
      "| 454396553776035072|          86|        12|  2019-01-02|    2019-03-25|   Amsterdam|      Sydney|     283|BOOKED|\n",
      "|5401900986077032623|         433|         9|  2019-01-02|    2019-04-16|     Beijing|    Brussels|     150|BOOKED|\n",
      "|2982690493231102623|         253|         4|  2019-01-02|    2019-03-18|      Sydney|       Osaka|     669|BOOKED|\n",
      "|3303592101252476893|         265|        16|  2019-01-02|    2019-04-26|     Beijing|      Prague|     155|BOOKED|\n",
      "|6464899145911761461|         201|         2|  2019-01-02|    2019-03-13|      Sydney|Buenos Aires|     757|BOOKED|\n",
      "|9019495296573081408|         496|        22|  2019-01-02|    2019-03-28|     Toronto|      Sydney|     383|BOOKED|\n",
      "|5501611409415570810|         424|        25|  2019-01-02|    2019-03-29|     Toronto|       Paris|     785|BOOKED|\n",
      "|8649467720458286245|          82|         8|  2019-01-02|    2019-03-10|    New York|      Sydney|     584|BOOKED|\n",
      "|3817049762679670509|         287|        13|  2019-01-02|    2019-04-27|      Prague|      Sydney|     358|BOOKED|\n",
      "| 837449549061349558|         255|         6|  2019-01-02|    2019-04-10|      Sydney|Johannesburg|      48|BOOKED|\n",
      "|3942300299786576698|          62|        13|  2019-01-02|    2019-03-24|    Lausanne|   Sao Paolo|     264|BOOKED|\n",
      "|1346351061412184184|         434|        10|  2019-01-03|    2019-03-26|Buenos Aires|      Lisbon|     798|BOOKED|\n",
      "|6503402509505848653|         433|         9|  2019-01-03|    2019-04-28|   Amsterdam|      Sydney|      58|BOOKED|\n",
      "|1906536806489130589|         308|         9|  2019-01-03|    2019-03-18|      Munich|        Kiev|     312|BOOKED|\n",
      "|8782080557465736900|         432|         8|  2019-01-03|    2019-03-21|       Paris|      Madrid|     635|BOOKED|\n",
      "|8497779794626234700|         453|         4|  2019-01-03|    2019-04-03|      Sydney|   Sao Paolo|     477|BOOKED|\n",
      "|8769154323704930060|         179|         5|  2019-01-03|    2019-03-29|      Warsaw|        Oslo|     175|BOOKED|\n",
      "|2935079461280073667|         188|        14|  2019-01-03|    2019-04-07|       Paris|   Amsterdam|      50|BOOKED|\n",
      "+-------------------+------------+----------+------------+--------------+------------+------------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\r\n",
       "import org.apache.spark.SparkContext._\r\n",
       "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DateType, LongType}\r\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@63d51b85\r\n",
       "schemaStruct1: org.apache.spark.sql.types.StructType = StructType(StructField(booking_id,LongType,true), StructField(traveller_id,IntegerType,true), StructField(company_id,IntegerType,true), StructField(booking_date,DateType,true), StructField(departure_date,DateType,true), StructField(origin,StringType,true), StructField(destination,StringType,true), StructField(priceUSD,IntegerType,true), StructField(status,StringType,true))\r\n",
       "df: org.apache.spark.sql.DataFrame = [booking_id: bigint, traveller_id: int ... 7 more fields]\r\n",
       "expected..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, DateType, LongType};\n",
    "\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val schemaStruct1 = StructType(\n",
    "            StructField( \"booking_id\", LongType, true )::\n",
    "            StructField( \"traveller_id\", IntegerType, true )::\n",
    "            StructField( \"company_id\", IntegerType, true )::\n",
    "            StructField( \"booking_date\", DateType, true )::\n",
    "            StructField( \"departure_date\", DateType, true )::\n",
    "            StructField( \"origin\", StringType, true )::\n",
    "            StructField( \"destination\", StringType, true )::\n",
    "            StructField( \"priceUSD\", IntegerType, true )::\n",
    "            StructField( \"status\", StringType, true ) ::Nil)\n",
    "       \n",
    "val df = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\",\"false\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .schema( schemaStruct1 )\n",
    "    .load(\"/Users/admin/Documents/CWT_DataEngineerCaseStudy/bookings.csv\")\n",
    "df.show()\n",
    "\n",
    "val expectedDF = sqlContext.read.format(\"com.databricks.spark.csv\")\n",
    "    .option(\"header\",\"false\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\n",
    "    .schema( schemaStruct1 )\n",
    "    .load(\"/Users/admin/Documents/CWT_DataEngineerCaseStudy/bookings.csv\")\n",
    "\n",
    "//unit testing the dataframe \n",
    "assert(df.count() == 16546) //16546\n",
    "val resultDF = df.except(expectedDF)\n",
    "assert(resultDF.rdd.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+----------+------------+--------------+--------+-----------+--------+---------+\n",
      "|        booking_id|traveller_id|company_id|booking_date|departure_date|  origin|destination|priceUSD|   status|\n",
      "+------------------+------------+----------+------------+--------------+--------+-----------+--------+---------+\n",
      "|621196447552601573|         416|        17|  2021-08-24|    2021-09-22|Brussels|     Sydney|      21|EXCHANGED|\n",
      "+------------------+------------+----------+------------+--------------+--------+-----------+--------+---------+\n",
      "\n",
      "Booking has already Exchanged Cancellation not possible\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import spark.sqlContext.implicits._\r\n",
       "arg1: Long = 621196447552601573\r\n",
       "arg2: Int = 416\r\n",
       "arg3: String = cancel\r\n",
       "arg4: org.apache.spark.sql.Column = current_date()\r\n",
       "locateDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [booking_id: bigint, traveller_id: int ... 7 more fields]\r\n",
       "canceleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [booking_id: bigint, traveller_id: int ... 7 more fields]\r\n",
       "res1: Any = ()\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "//val arg1 = 2188183561171387127L\n",
    "//val arg2 = 334\n",
    "\n",
    "val arg1 = 621196447552601573L\n",
    "val arg2 = 416\n",
    "val arg3 = \"cancel\"     // cancel or alter \n",
    "val arg4 = current_date //  date for exchange / cancel\n",
    "\n",
    "val locateDF = df.filter($\"departure_date\" > arg4 && $\"booking_id\" === arg1 && $\"traveller_id\" === arg2)\n",
    "                 .orderBy(desc(\"booking_date\")).limit(1)\n",
    "\n",
    "val canceleDF = locateDF.filter($\"status\"===\"EXCHANGED\")\n",
    "\n",
    "locateDF.show()\n",
    "\n",
    "if (locateDF.rdd.isEmpty) {\n",
    "    println(\"Record not found\")\n",
    "}\n",
    "\n",
    "else if (arg3 == \"cancel\" && canceleDF.count() > 0){\n",
    "    println(\"Booking has already Exchanged Cancellation not possible\")\n",
    "} \n",
    "\n",
    "else if (arg3 == \"cancel\" ){\n",
    "    val cancelDF = locateDF.withColumn(\"status\",when ($\"status\"===\"BOOKED\", \"CANCELLED\"))\n",
    "                           .withColumn(\"priceUSD\", (($\"priceUSD\"*20/100)-$\"priceUSD\"))\n",
    "                           .withColumn(\"booking_date\", arg4)\n",
    "    cancelDF.show()\n",
    "    df.union(cancelDF)\n",
    "} \n",
    "else if (arg3 == \"alter\" ){\n",
    "    val alterDF = locateDF.withColumn(\"status\",when (($\"status\"===\"BOOKED\")||($\"status\"===\"EXCHANGED\") , \"EXCHANGED\"))\n",
    "                          .withColumn(\"priceUSD\", ($\"priceUSD\"*20/100))\n",
    "                          .withColumn(\"departure_date\", arg4)\n",
    "    alterDF.show()\n",
    "    df.union(alterDF)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addcolDF: org.apache.spark.sql.DataFrame = [booking_id: bigint, traveller_id: int ... 9 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//generate 2 new fields called ‘year’ and ‘month’ of the booking date and saving the result to a file\n",
    "\n",
    "val addcolDF = df.withColumn(\"year\",year($\"booking_date\")).withColumn(\"month\",month($\"booking_date\"))\n",
    "addcolDF.write.option(\"header\",true)\n",
    "              .option(\"delimiter\",\",\")\n",
    "              .format(\"csv\").save(\"/Users/admin/Documents/CWT_DataEngineerCaseStudy/file_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|company_id|total_cost|\n",
      "+----------+----------+\n",
      "|        15|    332968|\n",
      "|        20|    300703|\n",
      "|         9|    289818|\n",
      "|        18|    279874|\n",
      "|         1|    271974|\n",
      "+----------+----------+\n",
      "\n",
      "+-----------+-----+\n",
      "|destination|trips|\n",
      "+-----------+-----+\n",
      "|     Sydney| 1250|\n",
      "|  Amsterdam|  966|\n",
      "|      Paris|  808|\n",
      "|  Sao Paolo|  621|\n",
      "|  Hong Kong|  371|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+----+\n",
      "|destination| top|\n",
      "+-----------+----+\n",
      "|     Sydney|2066|\n",
      "|  Amsterdam|1613|\n",
      "|      Paris|1348|\n",
      "|  Sao Paolo|1023|\n",
      "|  Hong Kong| 639|\n",
      "|       Oslo| 408|\n",
      "|  Stockholm| 384|\n",
      "|      Dubai| 371|\n",
      "|     Prague| 370|\n",
      "|     Athens| 353|\n",
      "|     London| 350|\n",
      "|    Toronto| 347|\n",
      "|      Cairo| 346|\n",
      "|     Berlin| 343|\n",
      "|     Moscow| 340|\n",
      "|     Milano| 338|\n",
      "|     Lisbon| 336|\n",
      "|     Madrid| 335|\n",
      "|Los Angeles| 332|\n",
      "|       Kiev| 330|\n",
      "+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "top5companies: org.apache.spark.sql.DataFrame = [company_id: int, total_cost: bigint]\r\n",
       "top5destinations: org.apache.spark.sql.DataFrame = [destination: string, trips: bigint]\r\n",
       "alldestinations: org.apache.spark.sql.DataFrame = [destination: string, top: bigint]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"JBTbookings\")\n",
    "val top5companies = sqlContext.sql(\"\"\"SELECT company_id,sum(priceUSD)total_cost FROM JBTbookings \n",
    "                                    group by company_id order by total_cost desc limit 5\"\"\")\n",
    "val top5destinations = sqlContext.sql(\"\"\"\n",
    "                                        select destination,count(destination) trips from (\n",
    "                                        SELECT booking_id,destination,count(destination)top FROM JBTbookings\n",
    "                                        where status = 'BOOKED' or status = 'EXCHANGED'\n",
    "                                        group by booking_id,destination \n",
    "                                        )  as t\n",
    "                                        group by destination order by trips desc limit 5\n",
    "                                      \"\"\")\n",
    "\n",
    "val alldestinations = sqlContext.sql(\"SELECT destination,count(destination)top FROM JBTbookings group by destination order by top desc \")\n",
    "\n",
    "// top 5 companies by based on the cost of travel\n",
    "top5companies.show()\n",
    "\n",
    "// top 5 popular destinations based on trips\n",
    "top5destinations.show()\n",
    "\n",
    "// all destinations w.r.t to visits\n",
    "alldestinations.show()              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
